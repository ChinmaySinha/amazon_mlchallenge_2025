{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566549de-5cbe-4c5b-a6ce-9b04d74ce0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (2.8.0+cu128)\n",
      "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.10.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7dd3e5-04bd-49a1-867d-80c8a0db126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# --- Machine Learning ---\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Deep Learning (for Embeddings) ---\n",
    "import torch\n",
    "import timm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# --- Setup ---\n",
    "# Set up the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"Setup complete. Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2eeb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing._data import StandardScaler as InternalStandardScaler\n",
    "torch.serialization.add_safe_globals([InternalStandardScaler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313119ed-cadb-4ad4-be24-51575e6b70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Data ---\n",
      "Shape: (75000, 4)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "\n",
      "--- Test Data ---\n",
      "Shape: (75000, 3)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
      "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
      "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
      "2  https://m.media-amazon.com/images/I/61KCM61J8e...  \n",
      "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...  \n",
      "4  https://m.media-amazon.com/images/I/71QYlrOMoS...  \n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('dataset/train.csv')\n",
    "    test_df = pd.read_csv('dataset/test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Please make sure train.csv and test.csv are in a 'dataset' folder.\")\n",
    "    # Create dummy dataframes to allow the rest of the notebook to run for demonstration\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "print(\"--- Training Data ---\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Test Data ---\")\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfec8ffa-c49f-4926-8a3f-2b182840fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing catalog_content for train and test sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing and encoding complete.\n",
      "\n",
      "Sample parsed data:\n",
      "   sample_id  Pack_Size  Value   Unit  Unit_Encoded\n",
      "0      33127          6  72.00     Fl            20\n",
      "1     198967          4  32.00  Ounce            40\n",
      "2     261251          6  11.40  Ounce            40\n",
      "3      55858          1  11.25  Ounce            40\n",
      "4     292686          1  12.00  Count            16\n"
     ]
    }
   ],
   "source": [
    "def parse_catalog_content(text):\n",
    "    # Pack Size: Look for (Pack of X)\n",
    "    pack_size_match = re.search(r'\\(Pack of (\\d+)\\)', text, re.IGNORECASE)\n",
    "    pack_size = int(pack_size_match.group(1)) if pack_size_match else 1\n",
    "\n",
    "    # Value: Look for Value: X\n",
    "    value_match = re.search(r'Value: ([\\d.]+)', text)\n",
    "    value = float(value_match.group(1)) if value_match else np.nan\n",
    "\n",
    "    # Unit: Look for Unit: X\n",
    "    unit_match = re.search(r'Unit: ([a-zA-Z]+)', text)\n",
    "    unit = unit_match.group(1) if unit_match else 'Unknown'\n",
    "\n",
    "    return pack_size, value, unit\n",
    "\n",
    "# Apply the parsing function\n",
    "print(\"Parsing catalog_content for train and test sets...\")\n",
    "train_df[['Pack_Size', 'Value', 'Unit']] = train_df['catalog_content'].apply(\n",
    "    lambda x: pd.Series(parse_catalog_content(x))\n",
    ")\n",
    "test_df[['Pack_Size', 'Value', 'Unit']] = test_df['catalog_content'].apply(\n",
    "    lambda x: pd.Series(parse_catalog_content(x))\n",
    ")\n",
    "\n",
    "# Encode Unit (fit on all units)\n",
    "all_units = pd.concat([train_df['Unit'], test_df['Unit']]).astype(str).unique()\n",
    "unit_encoder = LabelEncoder()\n",
    "unit_encoder.fit(all_units)\n",
    "\n",
    "train_df['Unit_Encoded'] = unit_encoder.transform(train_df['Unit'].astype(str))\n",
    "test_df['Unit_Encoded'] = unit_encoder.transform(test_df['Unit'].astype(str))\n",
    "\n",
    "print(\"Parsing and encoding complete.\")\n",
    "print(f\"\\nSample parsed data:\")\n",
    "print(train_df[['sample_id', 'Pack_Size', 'Value', 'Unit', 'Unit_Encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca95bf3-afe3-450f-a4db-a418605c45a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Generating Text Embeddings\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings for training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dad038155e941dfa47de096cc619808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings for test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3361026cf04e00a879fbf682fdd277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text embedding shape: (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Text Embeddings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the sentence transformer model\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Generate embeddings for the training data\n",
    "print(\"Generating text embeddings for training data...\")\n",
    "train_text_embeddings = text_model.encode(\n",
    "    train_df['catalog_content'].tolist(), \n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Generate embeddings for the test data\n",
    "print(\"Generating text embeddings for test data...\")\n",
    "test_text_embeddings = text_model.encode(\n",
    "    test_df['catalog_content'].tolist(), \n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_text_embed_df = pd.DataFrame(\n",
    "    train_text_embeddings, \n",
    "    columns=[f'txt_{i}' for i in range(train_text_embeddings.shape[1])]\n",
    ")\n",
    "test_text_embed_df = pd.DataFrame(\n",
    "    test_text_embeddings, \n",
    "    columns=[f'txt_{i}' for i in range(test_text_embeddings.shape[1])]\n",
    ")\n",
    "\n",
    "print(f\"\\nText embedding shape: {train_text_embed_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bc7aa7-5b76-4bf7-90a8-945fdacb05da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Generating Image Embeddings\n",
      "============================================================\n",
      "Generating image embeddings for training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f73d91c7fc14db083024fe5e2ef302e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating image embeddings for test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3c2937c2184046992fcc019ce545f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image embedding shape: (75000, 1280)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Image Embeddings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Image Model Setup\n",
    "img_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg').to(device)\n",
    "img_model.eval()\n",
    "config = img_model.default_cfg\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['input_size'][1:]),\n",
    "    transforms.CenterCrop(config['input_size'][1:]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config['mean'], std=config['std']),\n",
    "])\n",
    "\n",
    "# Image Feature Extraction Function\n",
    "def get_image_embedding(sample_id, model, device, transform, image_folder='images'):\n",
    "    image_path = os.path.join(image_folder, f\"{sample_id}.jpg\")\n",
    "    if not os.path.exists(image_path):\n",
    "        return np.zeros(1280)\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        batch_img = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(batch_img)\n",
    "        return embedding.cpu().numpy().flatten()\n",
    "    except Exception:\n",
    "        return np.zeros(1280)\n",
    "\n",
    "# Generate Image Embeddings\n",
    "print(\"Generating image embeddings for training data...\")\n",
    "train_image_embeddings = train_df['sample_id'].progress_apply(\n",
    "    lambda x: get_image_embedding(x, img_model, device, transform)\n",
    ")\n",
    "\n",
    "print(\"Generating image embeddings for test data...\")\n",
    "test_image_embeddings = test_df['sample_id'].progress_apply(\n",
    "    lambda x: get_image_embedding(x, img_model, device, transform)\n",
    ")\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_img_embed_df = pd.DataFrame(\n",
    "    train_image_embeddings.to_list(), \n",
    "    columns=[f'img_{i}' for i in range(1280)]\n",
    ")\n",
    "test_img_embed_df = pd.DataFrame(\n",
    "    test_image_embeddings.to_list(), \n",
    "    columns=[f'img_{i}' for i in range(1280)]\n",
    ")\n",
    "\n",
    "print(f\"\\nImage embedding shape: {train_img_embed_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8f0946a-a015-4078-83b8-73ee5aadb3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Combining All Features\n",
      "============================================================\n",
      "X_train shape: (75000, 1667)\n",
      "X_test shape: (75000, 1667)\n",
      "y_train_log shape: (75000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Combining All Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = ['Pack_Size', 'Value', 'Unit_Encoded']\n",
    "X_train_base = train_df[numerical_features].fillna(0)\n",
    "X_test_base = test_df[numerical_features].fillna(0)\n",
    "\n",
    "# Combine all features\n",
    "X_train = pd.concat([\n",
    "    X_train_base.reset_index(drop=True), \n",
    "    train_text_embed_df.reset_index(drop=True), \n",
    "    train_img_embed_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test = pd.concat([\n",
    "    X_test_base.reset_index(drop=True), \n",
    "    test_text_embed_df.reset_index(drop=True), \n",
    "    test_img_embed_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Prepare target variable - using log transform\n",
    "y_train = train_df['price']\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train_log shape: {y_train_log.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69725d35-7122-4a35-942c-cddf75dd39e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Creating Train/Validation Split\n",
      "============================================================\n",
      "Training split shape: (60000, 1667)\n",
      "Validation split shape: (15000, 1667)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating Train/Validation Split\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_split, X_val, y_train_log_split, y_val_log = train_test_split(\n",
    "    X_train, y_train_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training split shape: {X_train_split.shape}\")\n",
    "print(f\"Validation split shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59687182-60dc-49fe-8306-bb718d879f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4975f5a0-9396-4339-9a40-54a5c9275c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for price prediction\"\"\"\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X.values if isinstance(X, pd.DataFrame) else X)\n",
    "        self.y = torch.FloatTensor(y.values if y is not None else np.zeros(len(X)))\n",
    "        self.has_labels = y is not None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_labels:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "953ddbb3-c81b-4d3e-95da-63e1eb3dc0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for price prediction\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64], dropout=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "\n",
    "class AdvancedMultimodalMLP(nn.Module):\n",
    "    \"\"\"Advanced MLP with separate branches for different feature types\"\"\"\n",
    "    def __init__(self, num_features, text_features, img_features):\n",
    "        super(AdvancedMultimodalMLP, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.text_features = text_features\n",
    "        self.img_features = img_features\n",
    "        \n",
    "        # Numerical features branch\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Text features branch\n",
    "        self.text_net = nn.Sequential(\n",
    "            nn.Linear(text_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Image features branch\n",
    "        self.img_net = nn.Sequential(\n",
    "            nn.Linear(img_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_input_dim = 32 + 128 + 256\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split input into different modalities\n",
    "        num_feat = x[:, :self.num_features]\n",
    "        text_feat = x[:, self.num_features:self.num_features+self.text_features]\n",
    "        img_feat = x[:, self.num_features+self.text_features:]\n",
    "        \n",
    "        # Process each modality\n",
    "        num_out = self.num_net(num_feat)\n",
    "        text_out = self.text_net(text_feat)\n",
    "        img_out = self.img_net(img_feat)\n",
    "        \n",
    "        # Fuse and predict\n",
    "        combined = torch.cat([num_out, text_out, img_out], dim=1)\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62188471-4565-47b3-94f6-7872acfa3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.optim as optim\n",
    "def train_mlp(X_train, y_train, X_val, y_val, \n",
    "              model_type='simple',\n",
    "              num_epochs=100, \n",
    "              batch_size=256, \n",
    "              lr=0.001,\n",
    "              num_features=3,\n",
    "              text_features=384,\n",
    "              img_features=1280):\n",
    "    \"\"\"Train MLP model\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_type.upper()} MLP Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PriceDataset(X_train_scaled, y_train)\n",
    "    val_dataset = PriceDataset(X_val_scaled, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    if model_type == 'simple':\n",
    "        model = SimpleMLP(input_dim).to(device)\n",
    "    else:  # advanced\n",
    "        model = AdvancedMultimodalMLP(num_features, text_features, img_features).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5)   \n",
    "    \n",
    "    # Training loop\n",
    "    best_smape = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_smape = smape(np.expm1(all_targets), np.expm1(all_preds))\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "                  f\"Val SMAPE: {val_smape:.4f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_smape < best_smape:\n",
    "            best_smape = val_smape\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'scaler': scaler,\n",
    "                'best_smape': best_smape,\n",
    "            }, 'best_mlp_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_mlp_model.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    scaler = checkpoint['scaler']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best Validation SMAPE: {best_smape:.4f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, scaler, best_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc58da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training SIMPLE MLP Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,028,481\n",
      "Epoch [5/100] Train Loss: 0.6161 | Val Loss: 0.5806 | Val SMAPE: 59.2860%\n",
      "Epoch [10/100] Train Loss: 0.5143 | Val Loss: 0.5517 | Val SMAPE: 57.4907%\n",
      "Epoch [15/100] Train Loss: 0.4495 | Val Loss: 0.5367 | Val SMAPE: 56.0163%\n",
      "Epoch [20/100] Train Loss: 0.3998 | Val Loss: 0.5636 | Val SMAPE: 57.4143%\n",
      "Epoch [25/100] Train Loss: 0.3444 | Val Loss: 0.5292 | Val SMAPE: 54.7974%\n",
      "Epoch [30/100] Train Loss: 0.3168 | Val Loss: 0.5297 | Val SMAPE: 54.5436%\n",
      "Epoch [35/100] Train Loss: 0.2845 | Val Loss: 0.5355 | Val SMAPE: 54.5919%\n",
      "Epoch [40/100] Train Loss: 0.2765 | Val Loss: 0.5406 | Val SMAPE: 54.7032%\n",
      "Epoch [45/100] Train Loss: 0.2677 | Val Loss: 0.5452 | Val SMAPE: 54.8856%\n",
      "Epoch [50/100] Train Loss: 0.2629 | Val Loss: 0.5336 | Val SMAPE: 54.1619%\n",
      "Epoch [55/100] Train Loss: 0.2599 | Val Loss: 0.6008 | Val SMAPE: 57.7154%\n",
      "Epoch [60/100] Train Loss: 0.2575 | Val Loss: 0.5331 | Val SMAPE: 54.1026%\n",
      "Epoch [65/100] Train Loss: 0.2572 | Val Loss: 0.5350 | Val SMAPE: 54.0778%\n",
      "Epoch [70/100] Train Loss: 0.2579 | Val Loss: 0.5364 | Val SMAPE: 54.3086%\n",
      "\n",
      "Early stopping at epoch 72\n",
      "\n",
      "============================================================\n",
      "Best Validation SMAPE: 54.0120%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp_simple, scaler_simple, smape_simple = train_mlp(\n",
    "    X_train_split, \n",
    "    y_train_log_split, \n",
    "    X_val, \n",
    "    y_val_log,\n",
    "    model_type='simple',\n",
    "    num_epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd6592f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training ADVANCED MLP Model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1,071,393\n",
      "Epoch [5/100] Train Loss: 0.5490 | Val Loss: 0.5798 | Val SMAPE: 58.6892%\n",
      "Epoch [10/100] Train Loss: 0.4702 | Val Loss: 0.5456 | Val SMAPE: 55.8585%\n",
      "Epoch [15/100] Train Loss: 0.4232 | Val Loss: 0.5557 | Val SMAPE: 57.7974%\n",
      "Epoch [20/100] Train Loss: 0.3836 | Val Loss: 0.5758 | Val SMAPE: 58.3562%\n",
      "Epoch [25/100] Train Loss: 0.3377 | Val Loss: 0.5207 | Val SMAPE: 53.3606%\n",
      "Epoch [30/100] Train Loss: 0.3233 | Val Loss: 0.5275 | Val SMAPE: 53.9032%\n",
      "Epoch [35/100] Train Loss: 0.3119 | Val Loss: 0.5255 | Val SMAPE: 54.1479%\n",
      "Epoch [40/100] Train Loss: 0.2892 | Val Loss: 0.5493 | Val SMAPE: 56.0553%\n",
      "\n",
      "Early stopping at epoch 40\n",
      "\n",
      "============================================================\n",
      "Best Validation SMAPE: 53.3606%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "mlp_advanced, scaler_advanced, smape_advanced = train_mlp(\n",
    "    X_train_split, \n",
    "    y_train_log_split, \n",
    "    X_val, \n",
    "    y_val_log,\n",
    "    model_type='advanced',\n",
    "    num_epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_features=3,  # Pack_Size, Value, Unit_Encoded\n",
    "    text_features=384,  # Text embedding size\n",
    "    img_features=1280  # Image embedding size\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
