{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Core Libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# --- Machine Learning ---\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Deep Learning (for Embeddings) ---\n",
    "import torch\n",
    "import timm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Setup ---\n",
    "# Set up the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f\"Setup complete. Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('dataset/train.csv')\n",
    "    test_df = pd.read_csv('dataset/test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Please make sure train.csv and test.csv are in a 'dataset' folder.\")\n",
    "    # Create dummy dataframes to allow the rest of the notebook to run for demonstration\n",
    "    train_df = pd.DataFrame() \n",
    "    test_df = pd.DataFrame()\n",
    "\n",
    "print(\"--- Training Data ---\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Test Data ---\")\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74345081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_catalog_content(text):\n",
    "    # Pack Size: Look for (Pack of X)\n",
    "    pack_size_match = re.search(r'\\(Pack of (\\d+)\\)', text, re.IGNORECASE)\n",
    "    pack_size = int(pack_size_match.group(1)) if pack_size_match else 1\n",
    "\n",
    "    # Value: Look for Value: X\n",
    "    value_match = re.search(r'Value: ([\\d.]+)', text)\n",
    "    value = float(value_match.group(1)) if value_match else np.nan\n",
    "\n",
    "    # Unit: Look for Unit: X (specifically letters to avoid capturing numbers)\n",
    "    unit_match = re.search(r'Unit: ([a-zA-Z]+)', text) # Corrected Regex\n",
    "    unit = unit_match.group(1) if unit_match else 'Unknown'\n",
    "\n",
    "    return pack_size, value, unit\n",
    "\n",
    "# Apply the parsing function to both dataframes\n",
    "print(\"Parsing catalog_content for train and test sets...\")\n",
    "train_df[['Pack_Size', 'Value', 'Unit']] = train_df['catalog_content'].apply(\n",
    "    lambda x: pd.Series(parse_catalog_content(x))\n",
    ")\n",
    "test_df[['Pack_Size', 'Value', 'Unit']] = test_df['catalog_content'].apply(\n",
    "    lambda x: pd.Series(parse_catalog_content(x))\n",
    ")\n",
    "print(\"Parsing complete.\")\n",
    "\n",
    "# --- FIX: Fit the encoder on ALL possible units ---\n",
    "print(\"Encoding the 'Unit' feature...\")\n",
    "# Combine units from both train and test sets to learn all possible labels\n",
    "all_units = pd.concat([train_df['Unit'], test_df['Unit']]).astype(str).unique()\n",
    "\n",
    "unit_encoder = LabelEncoder()\n",
    "unit_encoder.fit(all_units) # Fit on all unique units\n",
    "\n",
    "# Now transform train and test sets\n",
    "train_df['Unit_Encoded'] = unit_encoder.transform(train_df['Unit'].astype(str))\n",
    "test_df['Unit_Encoded'] = unit_encoder.transform(test_df['Unit'].astype(str))\n",
    "print(\"Encoding complete.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Data After Parsing ---\")\n",
    "print(train_df[['sample_id', 'Pack_Size', 'Value', 'Unit', 'Unit_Encoded']].head())\n",
    "\n",
    "print(\"\\n--- Test Data After Parsing ---\")\n",
    "print(test_df[['sample_id', 'Pack_Size', 'Value', 'Unit', 'Unit_Encoded']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3b: Create Log-Transformed Numerical Features\n",
    "\n",
    "# Apply log transform to skewed numerical features to help the model\n",
    "# We use log1p which handles zeros safely (log(1+x))\n",
    "for col in ['Pack_Size', 'Value']:\n",
    "    train_df[f'{col}_log'] = np.log1p(train_df[col])\n",
    "    test_df[f'{col}_log'] = np.log1p(test_df[col])\n",
    "\n",
    "print(\"--- Created Log-Transformed Features ---\")\n",
    "print(train_df[['Pack_Size', 'Pack_Size_log', 'Value', 'Value_log']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6709eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3c: Create a Clean Text Column for Embeddings\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove boilerplate patterns\n",
    "    text = re.sub(r'Item Name:', '', text)\n",
    "    text = re.sub(r'Bullet Point \\d+:', '', text)\n",
    "    text = re.sub(r'Value: [\\d.]+', '', text)\n",
    "    text = re.sub(r'Unit: \\w+', '', text)\n",
    "    text = re.sub(r'Product Description:', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "print(\"Creating clean text column...\")\n",
    "train_df['clean_catalog_content'] = train_df['catalog_content'].apply(clean_text)\n",
    "test_df['clean_catalog_content'] = test_df['catalog_content'].apply(clean_text)\n",
    "\n",
    "print(\"--- Sample Cleaned Text ---\")\n",
    "print(train_df['clean_catalog_content'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92651cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3d: Advanced Feature Engineering (Brand, Item Size, Text Length) - CORRECTED\n",
    "\n",
    "# --- 1. Extract Brand Name (with robust error handling) ---\n",
    "\n",
    "def extract_brand(item_name):\n",
    "    \"\"\"Extracts the first word of an item name as the brand.\"\"\"\n",
    "    try:\n",
    "        brand = item_name.split()[0].upper()\n",
    "        return brand\n",
    "    except (IndexError, AttributeError):\n",
    "        # Handles empty or invalid item_name strings\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "def get_brand_from_catalog(catalog_text):\n",
    "    \"\"\"Safely finds the 'Item Name' section and extracts the brand.\"\"\"\n",
    "    # THIS IS THE FIX: Check if 'Item Name:' exists first\n",
    "    if 'Item Name:' in catalog_text:\n",
    "        # If it exists, proceed with the original logic\n",
    "        item_name_section = catalog_text.split('Item Name:')[1]\n",
    "        item_name = item_name_section.split('\\n')[0].strip()\n",
    "        return extract_brand(item_name)\n",
    "    else:\n",
    "        # If it doesn't exist, return a default value\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "print(\"Extracting brand names...\")\n",
    "# Apply our new, safer function to the catalog_content\n",
    "train_df['brand'] = train_df['catalog_content'].apply(get_brand_from_catalog)\n",
    "test_df['brand'] = test_df['catalog_content'].apply(get_brand_from_catalog)\n",
    "\n",
    "# Encode the new 'brand' feature\n",
    "all_brands = pd.concat([train_df['brand'], test_df['brand']]).astype(str).unique()\n",
    "brand_encoder = LabelEncoder().fit(all_brands)\n",
    "train_df['brand_encoded'] = brand_encoder.transform(train_df['brand'].astype(str))\n",
    "test_df['brand_encoded'] = brand_encoder.transform(test_df['brand'].astype(str))\n",
    "print(\"Brand feature created.\")\n",
    "\n",
    "\n",
    "# --- 2. Create Interaction and Text Statistic Features (no changes here) ---\n",
    "print(\"Creating interaction and text statistic features...\")\n",
    "train_df['item_size'] = train_df['Value'] / (train_df['Pack_Size'] + 1e-6)\n",
    "test_df['item_size'] = test_df['Value'] / (test_df['Pack_Size'] + 1e-6)\n",
    "train_df['desc_length'] = train_df['clean_catalog_content'].str.len()\n",
    "test_df['desc_length'] = test_df['clean_catalog_content'].str.len()\n",
    "print(\"Additional features created.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Sample of New Features ---\")\n",
    "print(train_df[['brand', 'brand_encoded', 'item_size', 'desc_length']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdcc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence transformer model\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Generate embeddings for the training data\n",
    "print(\"Generating text embeddings for training data...\")\n",
    "train_text_embeddings = text_model.encode(train_df['catalog_content'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Generate embeddings for the test data\n",
    "print(\"Generating text embeddings for test data...\")\n",
    "test_text_embeddings = text_model.encode(test_df['catalog_content'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_text_embed_df = pd.DataFrame(train_text_embeddings, columns=[f'txt_{i}' for i in range(train_text_embeddings.shape[1])])\n",
    "test_text_embed_df = pd.DataFrame(test_text_embeddings, columns=[f'txt_{i}' for i in range(test_text_embeddings.shape[1])])\n",
    "\n",
    "\n",
    "print(f\"\\nText embedding shape for training data: {train_text_embed_df.shape}\")\n",
    "print(\"Sample text embedding DataFrame:\")\n",
    "print(train_text_embed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate Image Embeddings from Local Files\n",
    "\n",
    "# --- Check if the image folder exists ---\n",
    "IMAGE_FOLDER = 'images' \n",
    "if not os.path.exists(IMAGE_FOLDER):\n",
    "    print(f\" ERROR: The '{IMAGE_FOLDER}' directory was not found.\")\n",
    "    print(\"Please run the image download cell first.\")\n",
    "else:\n",
    "    print(f\" Image folder '{IMAGE_FOLDER}' found. Proceeding with embedding.\")\n",
    "\n",
    "# --- Image Model Setup (remains the same) ---\n",
    "img_model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg').to(device)\n",
    "img_model.eval()\n",
    "config = img_model.default_cfg\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config['input_size'][1:]),\n",
    "    transforms.CenterCrop(config['input_size'][1:]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=config['mean'], std=config['std']),\n",
    "])\n",
    "\n",
    "# --- Image Feature Extraction Function ---\n",
    "def get_image_embedding(sample_id, model, device, transform, image_folder=IMAGE_FOLDER):\n",
    "    image_path = os.path.join(image_folder, f\"{sample_id}.jpg\")\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        return np.zeros(1280)\n",
    "        \n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        batch_img = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model(batch_img)\n",
    "        return embedding.cpu().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        return np.zeros(1280)\n",
    "\n",
    "# --- Generate Image Embeddings ---\n",
    "print(\"\\nGenerating image embeddings for training data...\")\n",
    "train_image_embeddings = train_df['sample_id'].progress_apply(\n",
    "    lambda x: get_image_embedding(x, img_model, device, transform)\n",
    ")\n",
    "\n",
    "print(\"Generating image embeddings for test data...\")\n",
    "test_image_embeddings = test_df['sample_id'].progress_apply(\n",
    "    lambda x: get_image_embedding(x, img_model, device, transform)\n",
    ")\n",
    "\n",
    "# --- Convert to DataFrames ---\n",
    "train_img_embed_df = pd.DataFrame(train_image_embeddings.to_list(), columns=[f'img_{i}' for i in range(1280)])\n",
    "test_img_embed_df = pd.DataFrame(test_image_embeddings.to_list(), columns=[f'img_{i}' for i in range(1280)])\n",
    "\n",
    "print(f\"\\nImage embedding shape: {train_img_embed_df.shape}\")\n",
    "print(\"Sample of new image embedding DataFrame (should NOT be all zeros):\")\n",
    "print(train_img_embed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b956b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Combining All Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define the complete list of all engineered numerical and categorical features\n",
    "numerical_features = [\n",
    "    'Pack_Size_log', \n",
    "    'Value_log', \n",
    "    'Unit_Encoded', \n",
    "    'brand_encoded', \n",
    "    'item_size',\n",
    "    'desc_length'\n",
    "]\n",
    "\n",
    "# --- NOTE: Add keyword features if you created them ---\n",
    "# If you have a cell that creates keyword features, this part includes them.\n",
    "# If not, you can comment out the following 3 lines.\n",
    "keywords = ['organic', 'premium', 'natural', 'gourmet', 'gluten-free', 'sugar-free', 'family size', 'bulk', 'value pack']\n",
    "keyword_cols = [f'kw_{k}' for k in keywords]\n",
    "# numerical_features.extend(keyword_cols) # Uncomment this if you have keyword features\n",
    "\n",
    "\n",
    "# Select the base numerical features, filling any potential NaNs with 0 for safety\n",
    "X_train_base = train_df[numerical_features].fillna(0)\n",
    "X_test_base = test_df[numerical_features].fillna(0)\n",
    "\n",
    "\n",
    "# Combine all feature sets into the final training matrix\n",
    "# reset_index(drop=True) is used to ensure a clean, continuous index for concatenation\n",
    "X_train = pd.concat([\n",
    "    X_train_base.reset_index(drop=True), \n",
    "    train_text_embed_df.reset_index(drop=True), \n",
    "    train_img_embed_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Combine all feature sets into the final test matrix\n",
    "X_test = pd.concat([\n",
    "    X_test_base.reset_index(drop=True), \n",
    "    test_text_embed_df.reset_index(drop=True), \n",
    "    test_img_embed_df.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Prepare the target variable, using a log transform for model stability\n",
    "y_train = train_df['price']\n",
    "y_train_log = np.log1p(y_train)\n",
    "\n",
    "# Print the final shapes and features used to verify everything is correct\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train_log shape: {y_train_log.shape}\")\n",
    "print(f\"\\nUsing the following {len(numerical_features)} numerical/categorical features:\")\n",
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d539bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating Train/Validation Split\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train_split, X_val, y_train_log_split, y_val_log = train_test_split(\n",
    "    X_train, y_train_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training split shape: {X_train_split.shape}\")\n",
    "print(f\"Validation split shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de923c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c770996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class PriceDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for price prediction\"\"\"\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X.values if isinstance(X, pd.DataFrame) else X)\n",
    "        self.y = torch.FloatTensor(y.values if y is not None else np.zeros(len(X)))\n",
    "        self.has_labels = y is not None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.has_labels:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ef972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"Simple MLP for price prediction\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 64], dropout=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "\n",
    "class AdvancedMultimodalMLP(nn.Module):\n",
    "    \"\"\"Advanced MLP with separate branches for different feature types\"\"\"\n",
    "    def __init__(self, num_features, text_features, img_features):\n",
    "        super(AdvancedMultimodalMLP, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.text_features = text_features\n",
    "        self.img_features = img_features\n",
    "        \n",
    "        # Numerical features branch\n",
    "        self.num_net = nn.Sequential(\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Text features branch\n",
    "        self.text_net = nn.Sequential(\n",
    "            nn.Linear(text_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Image features branch\n",
    "        self.img_net = nn.Sequential(\n",
    "            nn.Linear(img_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_input_dim = 32 + 128 + 256\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split input into different modalities\n",
    "        num_feat = x[:, :self.num_features]\n",
    "        text_feat = x[:, self.num_features:self.num_features+self.text_features]\n",
    "        img_feat = x[:, self.num_features+self.text_features:]\n",
    "        \n",
    "        # Process each modality\n",
    "        num_out = self.num_net(num_feat)\n",
    "        text_out = self.text_net(text_feat)\n",
    "        img_out = self.img_net(img_feat)\n",
    "        \n",
    "        # Fuse and predict\n",
    "        combined = torch.cat([num_out, text_out, img_out], dim=1)\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.optim as optim\n",
    "def train_mlp(X_train, y_train, X_val, y_val, \n",
    "              model_type='simple',\n",
    "              num_epochs=100, \n",
    "              batch_size=256, \n",
    "              lr=0.001,\n",
    "              num_features=3,\n",
    "              text_features=384,\n",
    "              img_features=1280):\n",
    "    \"\"\"Train MLP model\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_type.upper()} MLP Model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PriceDataset(X_train_scaled, y_train)\n",
    "    val_dataset = PriceDataset(X_val_scaled, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    if model_type == 'simple':\n",
    "        model = SimpleMLP(input_dim).to(device)\n",
    "    else:  # advanced\n",
    "        model = AdvancedMultimodalMLP(num_features, text_features, img_features).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5)   \n",
    "    \n",
    "    # Training loop\n",
    "    best_smape = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_targets.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        val_smape = smape(np.expm1(all_targets), np.expm1(all_preds))\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "                  f\"Val SMAPE: {val_smape:.4f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_smape < best_smape:\n",
    "            best_smape = val_smape\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'scaler': scaler,\n",
    "                'best_smape': best_smape,\n",
    "            }, 'best_mlp_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_mlp_model.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    scaler = checkpoint['scaler']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Best Validation SMAPE: {best_smape:.4f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return model, scaler, best_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3969e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp_simple, scaler_simple, smape_simple = train_mlp(\n",
    "    X_train_split, \n",
    "    y_train_log_split, \n",
    "    X_val, \n",
    "    y_val_log,\n",
    "    model_type='simple',\n",
    "    num_epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to call the training function (Corrected)\n",
    "\n",
    "# --- THIS IS THE FIX ---\n",
    "# We dynamically count the number of features instead of hardcoding it.\n",
    "\n",
    "# Define the numerical features list EXACTLY as you did in Cell 6\n",
    "numerical_features_list = [\n",
    "    'Pack_Size_log', \n",
    "    'Value_log', \n",
    "    'Unit_Encoded', \n",
    "    'brand_encoded', \n",
    "    'item_size',\n",
    "    'desc_length'\n",
    "]\n",
    "# Uncomment the line below if you also added keyword features in Cell 6\n",
    "# keywords = ['organic', 'premium', 'natural', 'gourmet', 'gluten-free', 'sugar-free', 'family size', 'bulk', 'value pack']\n",
    "# numerical_features_list.extend([f'kw_{k}' for k in keywords])\n",
    "\n",
    "num_features_count = len(numerical_features_list)\n",
    "text_features_count = train_text_embed_df.shape[1]\n",
    "img_features_count = train_img_embed_df.shape[1]\n",
    "\n",
    "print(f\"Using feature counts -> Numerical: {num_features_count}, Text: {text_features_count}, Image: {img_features_count}\")\n",
    "# --------------------------------\n",
    "\n",
    "mlp_advanced, scaler_advanced, smape_advanced = train_mlp(\n",
    "    X_train_split, \n",
    "    y_train_log_split, \n",
    "    X_val, \n",
    "    y_val_log,\n",
    "    model_type='advanced',\n",
    "    num_epochs=100,\n",
    "    batch_size=256,\n",
    "    lr=0.001,\n",
    "    num_features=num_features_count,    # <-- Use the correct count\n",
    "    text_features=text_features_count,  # <-- Use the correct count\n",
    "    img_features=img_features_count     # <-- Use the correct count\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
